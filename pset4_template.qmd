---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
PRVDR_CTGRY_SBTYP_CD - Provider Category Subtype Code
PRVDR_CTGRY_CD - Provider Category Code
PRVDR_NUM - CMS Certification Number
PGM_TRMNTN_CD - current termination status
CITY_NAME - City where the facility is located
STATE_CD - State code
ZIP_CD - Zip code of the facility
FAC_NAME - Facility name
ST_ADR - Street address

2. 
    a.
 ```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Define a function to load and filter data for short-term hospitals by year
def load_and_filter_pos(file_path, year):
    # Load the data with a specified encoding
    pos_data = pd.read_csv(file_path, low_memory=False, encoding='latin1')
    
    # Filter for short-term hospitals
    short_term_hospitals = pos_data[(pos_data['PRVDR_CTGRY_CD'] == 1) & 
                                    (pos_data['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    
    # Add a column for the year
    short_term_hospitals['Year'] = year
    print(f"Year {year} - Number of short-term hospitals: {len(short_term_hospitals)}")  # Check the count
    return short_term_hospitals

# Load and filter each year's dataset individually
pos2016 = load_and_filter_pos('pos2016.csv', 2016)
pos2017 = load_and_filter_pos('pos2017.csv', 2017)
pos2018 = load_and_filter_pos('pos2018.csv', 2018)
pos2019 = load_and_filter_pos('pos2019.csv', 2019)

# Append the datasets together
all_years_data = pd.concat([pos2016, pos2017, pos2018, pos2019], ignore_index=True)

# Count the number of observations by year in the combined data
observations_by_year = all_years_data['Year'].value_counts().sort_index()

# Plot the number of observations by year with more precise y-axis values
plt.figure(figsize=(10, 6))
observations_by_year.plot(kind='bar')
plt.title('Number of Short-Term Hospital Observations by Year')
plt.xlabel('Year')
plt.ylabel('Number of Observations')
plt.ylim(7240, 7305)
plt.yticks(range(7240, 7306, 5))

# Set y-axis with a smaller range of intervals to capture subtle differences
plt.yticks(range(min(observations_by_year) - 10, max(observations_by_year) + 10, 10))

plt.xticks(rotation=0)
plt.show()

```

4. 
    a.
```{python}
import pandas as pd
import matplotlib.pyplot as plt

unique_hospitals_by_year = all_years_data.groupby('Year')['PRVDR_NUM'].nunique()

# Plot the number of unique hospitals per year
plt.figure(figsize=(10, 6))
unique_hospitals_by_year.plot(kind='bar')
plt.title('Number of Unique Short-Term Hospitals by Year')
plt.xlabel('Year')
plt.ylabel('Number of Unique Hospitals')
plt.ylim(7240, 7305)  # Set a similar y-axis range for easy comparison
plt.xticks(rotation=0)
plt.show()

# Display the count for reference
print(unique_hospitals_by_year)

```

    b.
    Each CMS certification number (PRVDR_NUM) appears only once per year. This indicates that the dataset is structured so that each hospital has a single, unique record in each year, without duplicate entries or multiple records reflecting different operational statuses or characteristics within the same year. Since there are no duplicates within each year, the dataset is reliable for examining trends over time, such as changes in the number of active short-term hospitals each year. The increase in counts from 2016 to 2019 reflects an actual trend rather than potential data artifacts caused by duplicated records.

## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
def load_and_filter_pos(file_path, year):
    pos_data = pd.read_csv(file_path, low_memory=False, encoding='latin1')
    pos_data['Year'] = year
    return pos_data

pos2016 = load_and_filter_pos('pos2016.csv', 2016)
pos2017 = load_and_filter_pos('pos2017.csv', 2017)
pos2018 = load_and_filter_pos('pos2018.csv', 2018)
pos2019 = load_and_filter_pos('pos2019.csv', 2019)
all_years_data = pd.concat([pos2016, pos2017, pos2018, pos2019], ignore_index=True)
```
```{python}
pivoted_data = all_years_data.pivot_table(
    index=['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD'], 
    columns='Year', 
    values='PGM_TRMNTN_CD', 
    aggfunc='first'
).reset_index()
```
```{python}
pivoted_data['Suspected_Closure_Year'] = pivoted_data.apply(
    lambda row: next((year for year in [2017, 2018, 2019] if \
        (pd.isna(row[year]) or row[year] != 0) and row[2016] == 0), None), axis=1)
suspected_closures = pivoted_data.dropna(subset=['Suspected_Closure_Year'])
```
```{python}
suspected_closures_list = suspected_closures[['FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']]
total_suspected_closures = len(suspected_closures)
print(total_suspected_closures)
```
15730 hospital fit this definition
2. 
```{python}
sorted_suspected_closures = suspected_closures_list.sort_values(by='FAC_NAME')
sorted_suspected_closures.head(10)
```
3. 
    a.
    ```{python}
    zip_year_counts = all_years_data[all_years_data['PGM_TRMNTN_CD'] == 0].groupby(['ZIP_CD', 'Year']).size()
    ```
    ```{python}
    suspected_closures['Is_Merger'] = suspected_closures.apply(
    lambda row: (
        zip_year_counts.get((row['ZIP_CD'], row['Suspected_Closure_Year'] + 1), 0) >= 
        zip_year_counts.get((row['ZIP_CD'], row['Suspected_Closure_Year']), 0)
    ) if row['Suspected_Closure_Year'] < 2019 else False,
    axis=1)

    potential_mergers = suspected_closures[suspected_closures['Is_Merger']]
    corrected_closures = suspected_closures[~suspected_closures['Is_Merger']]
    ```
    b.
    ```{python}
    num_potential_mergers = len(potential_mergers)
    num_corrected_closures = len(corrected_closures)
    print("Number of hospitals identified as potential mergers:", num_potential_mergers)
    print("Number of hospitals remaining after correction:", num_corrected_closures)
    ```
    c.
    ```{python}
    corrected_closures_sorted = corrected_closures[['FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']].sort_values(by='FAC_NAME')
    corrected_closures_sorted.head(10)
    ```

## Download Census zip code shapefile (10 pt) 

1. 
    a.
    b. 
2. 

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
```{python}
# import sys
# sys.setrecursionlimit(200000) 

import geopandas as gpd
shapefile_path = 'gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp'
zips_all = gpd.read_file(shapefile_path)
```
```{python}
zips_all_centroids = zips_all.copy()
zips_all_centroids['centroid'] = zips_all.geometry.centroid
zips_all_centroids = zips_all_centroids.set_geometry('centroid')
```
```{python}
print("Dimensions of the GeoDataFrame:", zips_all_centroids.shape)
```

GEO_ID: A unique identifier for each area, formatted with a prefix (8600000US) plus the ZIP code.
ZCTA5: The 5-digit ZIP Code Tabulation Area (ZCTA), a Census Bureau representation of a ZIP code area.
NAME: The ZIP code associated with each ZCTA.
LSAD: Indicates that each row is a 5-digit ZCTA.
CENSUSAREA: The land area of the ZCTA in square miles.
geometry: The shape of each ZCTA, as either:
POLYGON: A single shape.
MULTIPOLYGON: Multiple shapes, used if a ZIP code area is split into separate parts.
2. 
```{python}
texas_zip_prefixes = ['75', '76', '77', '78', '79']  # Texas ZIP code prefixes
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(tuple(texas_zip_prefixes))]
unique_texas_zips = zips_texas_centroids['ZCTA5'].nunique()
```
```{python}
from shapely.ops import unary_union

texas_polygon = unary_union(zips_texas_centroids.geometry).convex_hull
# making this multipolygon into a big polygon with convex_hull
def polygons_intersect_or_touch(polygon1, polygon2):
    return polygon1.intersects(polygon2) or polygon1.touches(polygon2)
```
```{python}
zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids.geometry.apply(lambda x: polygons_intersect_or_touch(texas_polygon, x))]

unique_texas_border_zips = zips_texas_borderstates_centroids['ZCTA5'].nunique()
print("Number of unique ZIP codes in Texas:", unique_texas_zips)
print("Number of unique ZIP codes in Texas and border states:", unique_texas_border_zips)
```
3. 
```{python}
zips_texas_borderstates_centroids['ZCTA5'] = pd.to_numeric(zips_texas_borderstates_centroids['ZCTA5'], errors='coerce')

hospital_zip_codes_2016 = pos2016[['ZIP_CD']].drop_duplicates()
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    hospital_zip_codes_2016,
    how='inner',
    left_on='ZCTA5',
    right_on='ZIP_CD'
)
unique_hospital_zips = zips_withhospital_centroids['ZCTA5'].nunique()
print("Number of unique ZIP codes in Texas and bordering states with at least one hospital in 2016:", unique_hospital_zips)
```
Number of unique ZIP codes in Texas and bordering states with at least one hospital in 2016: 1292.

The merge function joins zips_texas_borderstates_centroids and hospital_zip_codes_2016:

how='inner': An inner join keeps only the rows where ZCTA5 in zips_texas_borderstates_centroids matches ZIP_CD in hospital_zip_codes_2016.

left_on='ZCTA5', right_on='ZIP_CD': Specifies the columns to match on.
4. 
```{python}
import time
from shapely.ops import nearest_points

subset_zips_texas_centroids = zips_texas_centroids.head(10)
def calculate_nearest_distance(row, target_gdf):
    nearest_geom = nearest_points(row.geometry, target_gdf.unary_union)[1]
    return row.geometry.distance(nearest_geom)

start_time = time.time()
subset_zips_texas_centroids['nearest_distance'] = subset_zips_texas_centroids.apply(
    calculate_nearest_distance, target_gdf=zips_withhospital_centroids, axis=1
)

elapsed_time = time.time() - start_time
print(f"Time taken for 10 ZIP codes: {elapsed_time:.2f} seconds")
```
```{python}
total_rows = len(zips_texas_centroids)
estimated_time = (elapsed_time / 10) * total_rows
print(f"Estimated time for full dataset: {estimated_time / 60:.2f} minutes")
```
    a.
```{python}
start_time = time.time()

zips_texas_centroids['nearest_distance'] = zips_texas_centroids.apply(
calculate_nearest_distance, target_gdf=zips_withhospital_centroids, axis=1
)

full_elapsed_time = time.time() - start_time
print(f"Time taken for full dataset: {full_elapsed_time / 60:.2f} minutes")
```
Estimated time for full dataset: 0.14 minutes
Time taken for full dataset: 0.08 minutes
They were quite off.
    b.
    The .prj file indicates that the coordinate system is GCS_North_American_1983, with units in degrees.
```{python}
zips_texas_centroids = zips_texas_centroids.to_crs("EPSG:5070")
zips_withhospital_centroids = zips_withhospital_centroids.to_crs("EPSG:5070")
zips_texas_centroids['nearest_distance_meters'] = zips_texas_centroids.apply(
    calculate_nearest_distance, target_gdf=zips_withhospital_centroids, axis=1
)
conversion_factor = 0.000621371  # 1 meter ≈ 0.000621371 miles
zips_texas_centroids['nearest_distance_miles'] = zips_texas_centroids['nearest_distance_meters'] * conversion_factor

average_distance_miles = zips_texas_centroids['nearest_distance_miles'].mean()
```
5. 
    a.
    In miles.
    b.
```{python}
print(f"Average distance to the nearest hospital for each ZIP code in Texas (in miles): {average_distance_miles:.2f}")
```
    c.
```{python}
import matplotlib.pyplot as plt

# Plot the distance to the nearest hospital for each ZIP code
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
zips_texas_centroids.plot(column='nearest_distance_miles', cmap='coolwarm', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)

ax.set_title("Distance to Nearest Hospital for Each ZIP Code in Texas (in miles)")
ax.set_axis_off()
plt.show()
```
This makes sense, as ZIP codes closer to the U.S. border tend to have greater distances to the nearest hospital compared to those located further inland.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 

1. 
2.The current approach identifies closures by checking if a hospital listed as “active” in 2016 is either not active or missing in later years. It adjusts for some mergers by removing suspected closures if the number of active hospitals in the ZIP code doesn’t decrease. It may misidentify closures if hospitals close and reopen under new certifications and in neighboring ZIP codes. It doesn’t account for population size, so it treats closures in densely populated and sparsely populated areas the same. Some improvement can be weighting closures by the population of the affected ZIP code to better understand the impact, or check if a hospital reopens in a neighboring ZIP code within a short distance, which may indicate a relocation rather than a true closure.

